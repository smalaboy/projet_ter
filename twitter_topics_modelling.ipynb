{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smalaboy/projet_ter/blob/main/twitter_topics_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ip6GZ-WLKQI3"
      },
      "outputs": [],
      "source": [
        "# !wget -O twitter_training2.zip https://drive.google.com/u/0/uc?id=1EPin6POZkj1S4xBhRv_EYHmlCF_fO9tk&export=download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQfNkaUhV7ex",
        "outputId": "5e25e6ca-b8a3-4b7e-b3f7-e48787139c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t     spark-3.2.1-bin-hadoop2.7\t    training.csv\n",
            "sample_data  spark-3.2.1-bin-hadoop2.7.tgz\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mBRKYCkLoTL0"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9S4bre0QoY7O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwqS6t14o-jx",
        "outputId": "72146cfc-f7a7-4bae-8ae7-2239b63ed768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t     spark-3.2.1-bin-hadoop2.7\t    spark-3.2.1-bin-hadoop2.7.tgz.1\n",
            "sample_data  spark-3.2.1-bin-hadoop2.7.tgz  training.csv\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/datasets/twitter_training.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v39ANLWmprvJ",
        "outputId": "87b35ce5-5d79-4eba-92c5-aa6b99bda336"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/datasets/twitter_training.zip\n",
            "replace training.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: training.csv            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
        "from pyspark.sql.types import ArrayType, DoubleType, BooleanType"
      ],
      "metadata": {
        "id": "MywvLshn4CHS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType()\\\n",
        "              .add(\"No\", IntegerType(), True)\\\n",
        "              .add(\"id\", StringType(), True)\\\n",
        "              .add(\"datetime\", StringType(), True)\\\n",
        "              .add(\"query\", StringType(), True)\\\n",
        "              .add(\"user\", StringType(), True)\\\n",
        "              .add(\"text\", StringType(), True)\n",
        "dataset = spark.read.csv('training.csv', schema=schema)\n",
        "dataset.printSchema()\n",
        "dataset.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrhjZ3N1rOLI",
        "outputId": "b3ca80ce-7189-4593-86a4-fef5c8bf7983"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- No: integer (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- datetime: string (nullable = true)\n",
            " |-- query: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(No=0, id='1467810369', datetime='Mon Apr 06 22:19:45 PDT 2009', query='NO_QUERY', user='_TheSpecialOne_', text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"),\n",
              " Row(No=0, id='1467810672', datetime='Mon Apr 06 22:19:49 PDT 2009', query='NO_QUERY', user='scotthamilton', text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"),\n",
              " Row(No=0, id='1467810917', datetime='Mon Apr 06 22:19:53 PDT 2009', query='NO_QUERY', user='mattycus', text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds'),\n",
              " Row(No=0, id='1467811184', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='ElleCTF', text='my whole body feels itchy and like its on fire '),\n",
              " Row(No=0, id='1467811193', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='Karoli', text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \")]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing tweets**"
      ],
      "metadata": {
        "id": "roalGWQWZecS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "import pyspark.sql.types as T\n"
      ],
      "metadata": {
        "id": "C0cY1DSKZdU2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes twitter handles\n",
        "def remove_users(tweet):\n",
        "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
        "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "yCU269Yp5tHZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes punctuation\n",
        "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n",
        "def remove_punctuation(tweet):\n",
        "    tweet = re.sub(f'[{punctuation}]+', ' ', tweet) \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "lCCjnuyUa6Zw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes numbers\n",
        "def remove_number(tweet):\n",
        "    tweet = re.sub('([0-9]+)', '', tweet) \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "pWIsbAECbacF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes hastags\n",
        "def remove_hashtag(tweet):\n",
        "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "Tv_HlLH4b0V5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_links(tweet):\n",
        "    tweet = re.sub(r'http\\S+', '', tweet) \n",
        "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) \n",
        "    tweet = tweet.strip('[link]') \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "F_iWbsgucQIz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User defined functions registration\n",
        "remove_users=udf(remove_users)\n",
        "remove_punctuation=udf(remove_punctuation)\n",
        "remove_number=udf(remove_number)\n",
        "remove_hashtag=udf(remove_hashtag)\n",
        "remove_links=udf(remove_links)"
      ],
      "metadata": {
        "id": "yx38KxoOcFMi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_tweets_1 = dataset.withColumn('processed_text', remove_users(dataset.text))\n",
        "processed_tweets_1 = processed_tweets_1.withColumn('processed_text', remove_punctuation(processed_tweets_1.processed_text))\n",
        "processed_tweets_1 = processed_tweets_1.withColumn('processed_text', remove_number(processed_tweets_1.processed_text))\n",
        "processed_tweets_1 = processed_tweets_1.withColumn('processed_text', remove_hashtag(processed_tweets_1.processed_text))\n",
        "processed_tweets_1 = processed_tweets_1.withColumn('processed_text', remove_links(processed_tweets_1.processed_text))"
      ],
      "metadata": {
        "id": "VxSqlxtVdlMq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_tweets_1.printSchema()\n",
        "processed_tweets_1.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOv5FdPoe-3E",
        "outputId": "754b7b57-9511-4646-e16f-e123483652f0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- No: integer (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- datetime: string (nullable = true)\n",
            " |-- query: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- processed_text: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(No=0, id='1467810369', datetime='Mon Apr 06 22:19:45 PDT 2009', query='NO_QUERY', user='_TheSpecialOne_', text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", processed_text=' http twitpic com yzl   Awww  that s a bummer   You shoulda got David Carr of Third Day to do it   D'),\n",
              " Row(No=0, id='1467810672', datetime='Mon Apr 06 22:19:49 PDT 2009', query='NO_QUERY', user='scotthamilton', text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", processed_text='s upset that he can t update his Facebook by texting it  and might cry as a result  School today also  Blah '),\n",
              " Row(No=0, id='1467810917', datetime='Mon Apr 06 22:19:53 PDT 2009', query='NO_QUERY', user='mattycus', text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', processed_text=' I dived many times for the ball  Managed to save    The rest go out of bounds'),\n",
              " Row(No=0, id='1467811184', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='ElleCTF', text='my whole body feels itchy and like its on fire ', processed_text='my whole body feels itchy and like its on fire '),\n",
              " Row(No=0, id='1467811193', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='Karoli', text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", processed_text=' no  it s not behaving at all  i m mad  why am i here  because I can t see you all over there  ')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import RegexTokenizer"
      ],
      "metadata": {
        "id": "8jdff2PTiSvH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and filter out words with len < 3\n",
        "tokenizer = RegexTokenizer().setPattern(\"[\\\\W_]+\").setMinTokenLength(3).setInputCol(\"processed_text\").setOutputCol(\"tokens\")"
      ],
      "metadata": {
        "id": "GSPKndnVhRea"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweets = tokenizer.transform(processed_tweets_1)"
      ],
      "metadata": {
        "id": "XzOrDUmdisvd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweets.printSchema()\n",
        "tokenized_tweets.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Vk6tCZi3Ga",
        "outputId": "f4ac9b60-5a08-4134-94ae-377768aba6a0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- No: integer (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- datetime: string (nullable = true)\n",
            " |-- query: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- processed_text: string (nullable = true)\n",
            " |-- tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(No=0, id='1467810369', datetime='Mon Apr 06 22:19:45 PDT 2009', query='NO_QUERY', user='_TheSpecialOne_', text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", processed_text=' http twitpic com yzl   Awww  that s a bummer   You shoulda got David Carr of Third Day to do it   D', tokens=['http', 'twitpic', 'com', 'yzl', 'awww', 'that', 'bummer', 'you', 'shoulda', 'got', 'david', 'carr', 'third', 'day']),\n",
              " Row(No=0, id='1467810672', datetime='Mon Apr 06 22:19:49 PDT 2009', query='NO_QUERY', user='scotthamilton', text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", processed_text='s upset that he can t update his Facebook by texting it  and might cry as a result  School today also  Blah ', tokens=['upset', 'that', 'can', 'update', 'his', 'facebook', 'texting', 'and', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah']),\n",
              " Row(No=0, id='1467810917', datetime='Mon Apr 06 22:19:53 PDT 2009', query='NO_QUERY', user='mattycus', text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', processed_text=' I dived many times for the ball  Managed to save    The rest go out of bounds', tokens=['dived', 'many', 'times', 'for', 'the', 'ball', 'managed', 'save', 'the', 'rest', 'out', 'bounds']),\n",
              " Row(No=0, id='1467811184', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='ElleCTF', text='my whole body feels itchy and like its on fire ', processed_text='my whole body feels itchy and like its on fire ', tokens=['whole', 'body', 'feels', 'itchy', 'and', 'like', 'its', 'fire']),\n",
              " Row(No=0, id='1467811193', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='Karoli', text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", processed_text=' no  it s not behaving at all  i m mad  why am i here  because I can t see you all over there  ', tokens=['not', 'behaving', 'all', 'mad', 'why', 'here', 'because', 'can', 'see', 'you', 'all', 'over', 'there'])]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8VCpxppjR5K",
        "outputId": "f7593ae3-8598-42d5-c271-fc25fb7a6298"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(row):\n",
        "    row = [lemmatizer.lemmatize(word,'v') for word in row]\n",
        "    return row\n",
        "\n",
        "lemmatization_udf = udf(lemmatize, T.ArrayType(T.StringType()))"
      ],
      "metadata": {
        "id": "_1KGG6PNjV8m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweets=tokenized_tweets.withColumn('tokens_lemma', lemmatization_udf(tokenized_tweets['tokens']))"
      ],
      "metadata": {
        "id": "RKl3d4oZkFNU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweets.printSchema()\n",
        "tokenized_tweets.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJZzDe9-kUqN",
        "outputId": "7494463a-4fc7-4628-9d79-8011f1c7d8cd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- No: integer (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- datetime: string (nullable = true)\n",
            " |-- query: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- processed_text: string (nullable = true)\n",
            " |-- tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- tokens_lemma: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(No=0, id='1467810369', datetime='Mon Apr 06 22:19:45 PDT 2009', query='NO_QUERY', user='_TheSpecialOne_', text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", processed_text=' http twitpic com yzl   Awww  that s a bummer   You shoulda got David Carr of Third Day to do it   D', tokens=['http', 'twitpic', 'com', 'yzl', 'awww', 'that', 'bummer', 'you', 'shoulda', 'got', 'david', 'carr', 'third', 'day'], tokens_lemma=['http', 'twitpic', 'com', 'yzl', 'awww', 'that', 'bummer', 'you', 'shoulda', 'get', 'david', 'carr', 'third', 'day']),\n",
              " Row(No=0, id='1467810672', datetime='Mon Apr 06 22:19:49 PDT 2009', query='NO_QUERY', user='scotthamilton', text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", processed_text='s upset that he can t update his Facebook by texting it  and might cry as a result  School today also  Blah ', tokens=['upset', 'that', 'can', 'update', 'his', 'facebook', 'texting', 'and', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah'], tokens_lemma=['upset', 'that', 'can', 'update', 'his', 'facebook', 'texting', 'and', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah']),\n",
              " Row(No=0, id='1467810917', datetime='Mon Apr 06 22:19:53 PDT 2009', query='NO_QUERY', user='mattycus', text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', processed_text=' I dived many times for the ball  Managed to save    The rest go out of bounds', tokens=['dived', 'many', 'times', 'for', 'the', 'ball', 'managed', 'save', 'the', 'rest', 'out', 'bounds'], tokens_lemma=['dive', 'many', 'time', 'for', 'the', 'ball', 'manage', 'save', 'the', 'rest', 'out', 'bound']),\n",
              " Row(No=0, id='1467811184', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='ElleCTF', text='my whole body feels itchy and like its on fire ', processed_text='my whole body feels itchy and like its on fire ', tokens=['whole', 'body', 'feels', 'itchy', 'and', 'like', 'its', 'fire'], tokens_lemma=['whole', 'body', 'feel', 'itchy', 'and', 'like', 'its', 'fire']),\n",
              " Row(No=0, id='1467811193', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='Karoli', text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", processed_text=' no  it s not behaving at all  i m mad  why am i here  because I can t see you all over there  ', tokens=['not', 'behaving', 'all', 'mad', 'why', 'here', 'because', 'can', 'see', 'you', 'all', 'over', 'there'], tokens_lemma=['not', 'behave', 'all', 'mad', 'why', 'here', 'because', 'can', 'see', 'you', 'all', 'over', 'there'])]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic modelling with grid search for the number of topics**"
      ],
      "metadata": {
        "id": "YBdyE0dTncP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml.feature import CountVectorizer"
      ],
      "metadata": {
        "id": "tUjA5MEGpZu3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countVectorizer = CountVectorizer()\n",
        "countVectorizer.setInputCol(\"tokens_lemma\")\n",
        "countVectorizer.setOutputCol(\"features\")\n",
        "vectorizerModel = countVectorizer.fit(tokenized_tweets)\n",
        "wordsVector = vectorizerModel.transform(tokenized_tweets)"
      ],
      "metadata": {
        "id": "82KCaU2ntCae"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordsVector.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQsM-9HIzjJz",
        "outputId": "e748ec62-6fee-415b-d1a4-565073e79563"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| No|        id|            datetime|   query|           user|                text|      processed_text|              tokens|        tokens_lemma|            features|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...| http twitpic com...|[http, twitpic, c...|[http, twitpic, c...|(262144,[2,6,7,17...|\n",
            "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|s upset that he c...|[upset, that, can...|[upset, that, can...|(262144,[1,7,13,2...|\n",
            "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...| I dived many tim...|[dived, many, tim...|[dive, many, time...|(262144,[0,5,21,2...|\n",
            "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|my whole body fee...|[whole, body, fee...|[whole, body, fee...|(262144,[1,20,43,...|\n",
            "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...| no  it s not beh...|[not, behaving, a...|[not, behave, all...|(262144,[2,11,13,...|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# num_topics=range(3,8)\n",
        "# models=[]\n",
        "# log_likeli=[]\n",
        "# log_perp=[]\n",
        "# for num in num_topics:\n",
        "#   lda = LDA(k=num, maxIter=50)\n",
        "#   ldaModel = lda.fit(wordsVector)\n",
        "#   models.append(ldaModel)\n",
        "#   ll = ldaModel.logLikelihood(wordsVector)\n",
        "#   lp = ldaModel.logPerplexity(wordsVector)\n",
        "#   log_likeli.append(ll)\n",
        "#   log_perp.append(lp)"
      ],
      "metadata": {
        "id": "D_5-EPdhnbAb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# plot_data=pd.DataFrame(list(zip(num_topics,log_likeli,log_perp)),\n",
        "#             columns=['topics_num','logLikelihood','logPerplexity'])    \n",
        "\n",
        "# plot_data.plot(x='topics_num',y='logLikelihood',kind = 'line')\n",
        "# plt.show()\n",
        "\n",
        "# plot_data.plot(x='topics_num',y='logPerplexity',kind = 'line')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "JR5C9ke3Uh1n"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LDA(k=6, maxIter=100)\n",
        "lda_model = lda.fit(wordsVector)"
      ],
      "metadata": {
        "id": "witM8HYey0eE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting vocabulary from CountVectorizer\n",
        "vocabulary = vectorizerModel.vocabulary\n",
        "\n",
        "# create topics based on LDA\n",
        "lda_topics = lda_model.describeTopics()\n",
        "lda_topics.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2h6tuOnzYN7",
        "outputId": "acf6d1bc-ce1c-4fd1-99eb-377161361da4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|topic|         termIndices|         termWeights|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|[24, 42, 118, 96,...|[0.14688533470031...|\n",
            "|    1|[2491, 0, 5108, 5...|[0.00429466665852...|\n",
            "|    2|[0, 1, 4, 3, 2, 5...|[0.03710505284451...|\n",
            "|    3|[3146, 3991, 0, 4...|[0.00311697690132...|\n",
            "|    4|[323, 799, 1055, ...|[0.05283965861080...|\n",
            "|    5|[2, 33, 5, 68, 12...|[0.08312545185625...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topics_words = lda_topics.rdd\\\n",
        "       .map(lambda row: row['termIndices'])\\\n",
        "       .map(lambda idx_list: [vocabulary[idx] for idx in idx_list])\\\n",
        "       .collect()\n",
        "\n",
        "for idx, topic in enumerate(topics_words):\n",
        "    print(\"topic: {}\".format(idx))\n",
        "    print(\"*\"*25)\n",
        "    for word in topic:\n",
        "       print(word)\n",
        "    print(\"*\"*25)\n",
        "    \n",
        "transformed = lda_model.transform(wordsVector)\n",
        "transformed.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KyM8RYGJ1AJj",
        "outputId": "45b56cc6-57b9-43b1-91e5-afee2bef11be"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: 0\n",
            "*************************\n",
            "http\n",
            "com\n",
            "twitpic\n",
            "bite\n",
            "tinyurl\n",
            "plurk\n",
            "blip\n",
            "the\n",
            "and\n",
            "photo\n",
            "*************************\n",
            "topic: 1\n",
            "*************************\n",
            "othing\n",
            "the\n",
            "homee\n",
            "for\n",
            "boredd\n",
            "ting\n",
            "with\n",
            "chillen\n",
            "wakey\n",
            "som\n",
            "*************************\n",
            "topic: 2\n",
            "*************************\n",
            "the\n",
            "and\n",
            "have\n",
            "be\n",
            "you\n",
            "for\n",
            "get\n",
            "that\n",
            "but\n",
            "just\n",
            "*************************\n",
            "topic: 3\n",
            "*************************\n",
            "dia\n",
            "bom\n",
            "the\n",
            "ana\n",
            "bind\n",
            "neither\n",
            "hai\n",
            "hugh\n",
            "gnight\n",
            "boa\n",
            "*************************\n",
            "topic: 4\n",
            "*************************\n",
            "welcome\n",
            "yup\n",
            "congratulations\n",
            "pleasure\n",
            "goodmorning\n",
            "thankyou\n",
            "que\n",
            "your\n",
            "you\n",
            "spongebob\n",
            "*************************\n",
            "topic: 5\n",
            "*************************\n",
            "you\n",
            "thank\n",
            "for\n",
            "twitter\n",
            "follow\n",
            "miss\n",
            "be\n",
            "know\n",
            "what\n",
            "hey\n",
            "*************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-d8c9abb82dd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsVector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtransformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed = lda_model.transform(wordsVector)\n",
        "transformed.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUzNm6vvd2gI",
        "outputId": "6703653f-d111-4c15-ce4d-f0cb03866609"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| No|        id|            datetime|   query|           user|                text|      processed_text|              tokens|        tokens_lemma|            features|   topicDistribution|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...| http twitpic com...|[http, twitpic, c...|[http, twitpic, c...|(262144,[2,6,7,17...|[0.24075470291006...|\n",
            "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|s upset that he c...|[upset, that, can...|[upset, that, can...|(262144,[1,7,13,2...|[0.00528633055907...|\n",
            "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...| I dived many tim...|[dived, many, tim...|[dive, many, time...|(262144,[0,5,21,2...|[0.00647815512888...|\n",
            "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|my whole body fee...|[whole, body, fee...|[whole, body, fee...|(262144,[1,20,43,...|[0.00926248927681...|\n",
            "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...| no  it s not beh...|[not, behaving, a...|[not, behave, all...|(262144,[2,11,13,...|[0.00602529166685...|\n",
            "|  0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...| not the whole crew |[not, the, whole,...|[not, the, whole,...|(262144,[0,11,386...|[0.01624417063085...|\n",
            "|  0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |         Need a hug |         [need, hug]|         [need, hug]|(262144,[59,430],...|[0.02606964686284...|\n",
            "|  0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...| hey  long time n...|[hey, long, time,...|[hey, long, time,...|(262144,[2,27,33,...|[0.11077603526432...|\n",
            "|  0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...| nope they didn t...|[nope, they, didn...|[nope, they, didn...|(262144,[4,56,119...|[0.01624413361692...|\n",
            "|  0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|     que me muera   |        [que, muera]|        [que, muera]|(262144,[2061,205...|[0.34654836258036...|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.save(\"lda_model\")"
      ],
      "metadata": {
        "id": "mpGhlqkrfn0T"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvf lda_model.tar lda_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkMl_TDFgaTI",
        "outputId": "e7f39aa0-d853-4412-8452-196610a9c336"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lda_model/\n",
            "lda_model/data/\n",
            "lda_model/data/part-00000-0316421a-2920-4def-a82e-d6e8aa3edabd-c000.snappy.parquet\n",
            "lda_model/data/._SUCCESS.crc\n",
            "lda_model/data/.part-00000-0316421a-2920-4def-a82e-d6e8aa3edabd-c000.snappy.parquet.crc\n",
            "lda_model/data/_SUCCESS\n",
            "lda_model/metadata/\n",
            "lda_model/metadata/.part-00000.crc\n",
            "lda_model/metadata/part-00000\n",
            "lda_model/metadata/._SUCCESS.crc\n",
            "lda_model/metadata/_SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordsVector.write.json(\"processed_data\")"
      ],
      "metadata": {
        "id": "n1HRwHU_hYWr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvf processed_data.tar processed_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnxtsu3AjDAj",
        "outputId": "089a6372-3b3a-4de3-8291-d2a9f665a0af"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed_data/\n",
            "processed_data/part-00000-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json\n",
            "processed_data/part-00001-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json\n",
            "processed_data/.part-00000-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json.crc\n",
            "processed_data/.part-00001-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json.crc\n",
            "processed_data/._SUCCESS.crc\n",
            "processed_data/_SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWhj9nPyqRMe",
        "outputId": "97d46f26-3cd4-4f4b-8b92-2a5bf04c1d1f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t\tprocessed_data.tar\t       spark-3.2.1-bin-hadoop2.7.tgz.1\n",
            "lda_model\tsample_data\t\t       training.csv\n",
            "lda_model.tar\tspark-3.2.1-bin-hadoop2.7\n",
            "processed_data\tspark-3.2.1-bin-hadoop2.7.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_eUBExB1BreN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LocalLDAModel, LDAModel, LDA"
      ],
      "metadata": {
        "id": "BJk71LVyC34Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvf lda_model.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwOVGNdiDZKF",
        "outputId": "e13c0bcf-4a51-4f15-e82e-34df1d3b7643"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lda_model/\n",
            "lda_model/data/\n",
            "lda_model/data/part-00000-0316421a-2920-4def-a82e-d6e8aa3edabd-c000.snappy.parquet\n",
            "lda_model/data/._SUCCESS.crc\n",
            "lda_model/data/.part-00000-0316421a-2920-4def-a82e-d6e8aa3edabd-c000.snappy.parquet.crc\n",
            "lda_model/data/_SUCCESS\n",
            "lda_model/metadata/\n",
            "lda_model/metadata/.part-00000.crc\n",
            "lda_model/metadata/part-00000\n",
            "lda_model/metadata/._SUCCESS.crc\n",
            "lda_model/metadata/_SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = LocalLDAModel.load(\"lda_model\")"
      ],
      "metadata": {
        "id": "wHQvhNpODIkb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pEVNgjGkEB-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.transform(wordsVector[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "dD38UM2u9trQ",
        "outputId": "ded979af-678a-4dd7-ccef-c6262ebf81d4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-d5045526ff5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordsVector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'LocalLDAModel' object is not subscriptable"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "twitter_topics_modelling.ipynb",
      "provenance": [],
      "mount_file_id": "1LrHvLtuROSvUjeW9qEFgUx4v4yGWYPVk",
      "authorship_tag": "ABX9TyMWivAyU4TPFgjtt6VchpKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}