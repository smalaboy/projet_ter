{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smalaboy/projet_ter/blob/main/twitter_topics_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ip6GZ-WLKQI3"
      },
      "outputs": [],
      "source": [
        "# !wget -O twitter_training2.zip https://drive.google.com/u/0/uc?id=1EPin6POZkj1S4xBhRv_EYHmlCF_fO9tk&export=download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQfNkaUhV7ex",
        "outputId": "5e25e6ca-b8a3-4b7e-b3f7-e48787139c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t     spark-3.2.1-bin-hadoop2.7\t    training.csv\n",
            "sample_data  spark-3.2.1-bin-hadoop2.7.tgz\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mBRKYCkLoTL0"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9S4bre0QoY7O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwqS6t14o-jx",
        "outputId": "72146cfc-f7a7-4bae-8ae7-2239b63ed768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t     spark-3.2.1-bin-hadoop2.7\t    spark-3.2.1-bin-hadoop2.7.tgz.1\n",
            "sample_data  spark-3.2.1-bin-hadoop2.7.tgz  training.csv\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/datasets/twitter_training.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v39ANLWmprvJ",
        "outputId": "d729be49-0bac-4df5-bc31-2a77e83af12c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/datasets/twitter_training.zip\n",
            "  inflating: training.csv            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
        "from pyspark.sql.types import ArrayType, DoubleType, BooleanType"
      ],
      "metadata": {
        "id": "MywvLshn4CHS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType()\\\n",
        "              .add(\"No\", IntegerType(), True)\\\n",
        "              .add(\"id\", StringType(), True)\\\n",
        "              .add(\"datetime\", StringType(), True)\\\n",
        "              .add(\"query\", StringType(), True)\\\n",
        "              .add(\"user\", StringType(), True)\\\n",
        "              .add(\"text\", StringType(), True)\n",
        "dataset = spark.read.csv('training.csv', schema=schema)\n",
        "dataset.printSchema()\n",
        "dataset.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrhjZ3N1rOLI",
        "outputId": "8a31f85d-4cb5-4b1c-cef4-2bde45d9a1cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- No: integer (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- datetime: string (nullable = true)\n",
            " |-- query: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(No=0, id='1467810369', datetime='Mon Apr 06 22:19:45 PDT 2009', query='NO_QUERY', user='_TheSpecialOne_', text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"),\n",
              " Row(No=0, id='1467810672', datetime='Mon Apr 06 22:19:49 PDT 2009', query='NO_QUERY', user='scotthamilton', text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"),\n",
              " Row(No=0, id='1467810917', datetime='Mon Apr 06 22:19:53 PDT 2009', query='NO_QUERY', user='mattycus', text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds'),\n",
              " Row(No=0, id='1467811184', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='ElleCTF', text='my whole body feels itchy and like its on fire '),\n",
              " Row(No=0, id='1467811193', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='Karoli', text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \")]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing tweets**"
      ],
      "metadata": {
        "id": "roalGWQWZecS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "import pyspark.sql.types as T\n",
        "from pyspark.ml.feature import StopWordsRemover\n"
      ],
      "metadata": {
        "id": "C0cY1DSKZdU2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes twitter handles\n",
        "def remove_users(tweet):\n",
        "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
        "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "yCU269Yp5tHZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes punctuation\n",
        "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n",
        "def remove_punctuation(tweet):\n",
        "    tweet = re.sub(f'[{punctuation}]+', ' ', tweet) \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "lCCjnuyUa6Zw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes numbers\n",
        "def remove_number(tweet):\n",
        "    tweet = re.sub('([0-9]+)', '', tweet) \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "pWIsbAECbacF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes hastags\n",
        "def remove_hashtag(tweet):\n",
        "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "Tv_HlLH4b0V5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_links(tweet):\n",
        "    tweet = re.sub(r'http\\S+', '', tweet) \n",
        "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) \n",
        "    tweet = tweet.strip('[link]') \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "F_iWbsgucQIz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User defined functions registration\n",
        "remove_users=udf(remove_users)\n",
        "remove_punctuation=udf(remove_punctuation)\n",
        "remove_number=udf(remove_number)\n",
        "remove_hashtag=udf(remove_hashtag)\n",
        "remove_links=udf(remove_links)"
      ],
      "metadata": {
        "id": "yx38KxoOcFMi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_tweets_1 = dataset.withColumn('processed_text', remove_users(dataset.text))\n",
        "processed_tweets_1 = processed_tweets_1.withColumn('processed_text', remove_punctuation(processed_tweets_1.processed_text))\n",
        "processed_tweets_1 = processed_tweets_1.withColumn('processed_text', remove_number(processed_tweets_1.processed_text))\n",
        "processed_tweets_1 = processed_tweets_1.withColumn('processed_text', remove_hashtag(processed_tweets_1.processed_text))\n",
        "processed_tweets_1 = processed_tweets_1.withColumn('processed_text', remove_links(processed_tweets_1.processed_text))"
      ],
      "metadata": {
        "id": "VxSqlxtVdlMq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "# print(stopwords.words('english')[:20])\n",
        "stopwords_list = list(set(StopWordsRemover().getStopWords() + stopwords.words('english') + [\"http\", \"https\"]))\n",
        "print(len(stopwords_list), stopwords_list[:20])\n",
        "stopWordsRemover = StopWordsRemover(stopWords=stopwords_list)\n",
        "# print(StopWordsRemover().getStopWords())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ImOPQkXk_k3",
        "outputId": "44c68f67-0a81-4697-fd48-46ce37821ddf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "213 ['was', \"let's\", 'from', 'how', 'all', 'same', 'aren', \"it's\", 'down', 'needn', 'after', 'an', \"weren't\", 'myself', 'through', 'own', 'him', 'whom', 're', \"doesn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZH_JcwvyptZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_tweets_1.printSchema()\n",
        "processed_tweets_1.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOv5FdPoe-3E",
        "outputId": "6eae6ebf-7e38-42ae-e128-7f736e5d7d9e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- No: integer (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- datetime: string (nullable = true)\n",
            " |-- query: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- processed_text: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(No=0, id='1467810369', datetime='Mon Apr 06 22:19:45 PDT 2009', query='NO_QUERY', user='_TheSpecialOne_', text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", processed_text=' http twitpic com yzl   Awww  that s a bummer   You shoulda got David Carr of Third Day to do it   D'),\n",
              " Row(No=0, id='1467810672', datetime='Mon Apr 06 22:19:49 PDT 2009', query='NO_QUERY', user='scotthamilton', text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", processed_text='s upset that he can t update his Facebook by texting it  and might cry as a result  School today also  Blah '),\n",
              " Row(No=0, id='1467810917', datetime='Mon Apr 06 22:19:53 PDT 2009', query='NO_QUERY', user='mattycus', text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', processed_text=' I dived many times for the ball  Managed to save    The rest go out of bounds'),\n",
              " Row(No=0, id='1467811184', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='ElleCTF', text='my whole body feels itchy and like its on fire ', processed_text='my whole body feels itchy and like its on fire '),\n",
              " Row(No=0, id='1467811193', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='Karoli', text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", processed_text=' no  it s not behaving at all  i m mad  why am i here  because I can t see you all over there  ')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import RegexTokenizer"
      ],
      "metadata": {
        "id": "8jdff2PTiSvH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and filter out words with len < 3\n",
        "tokenizer = RegexTokenizer().setPattern(\"[\\\\W_]+\").setMinTokenLength(4).setInputCol(\"processed_text\").setOutputCol(\"tokens\")"
      ],
      "metadata": {
        "id": "GSPKndnVhRea"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweets = tokenizer.transform(processed_tweets_1)"
      ],
      "metadata": {
        "id": "XzOrDUmdisvd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweets.printSchema()\n",
        "tokenized_tweets.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Vk6tCZi3Ga",
        "outputId": "ee57fb19-28e7-4eac-bf3a-4be84eb21c0a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- No: integer (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- datetime: string (nullable = true)\n",
            " |-- query: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- processed_text: string (nullable = true)\n",
            " |-- tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(No=0, id='1467810369', datetime='Mon Apr 06 22:19:45 PDT 2009', query='NO_QUERY', user='_TheSpecialOne_', text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", processed_text=' http twitpic com yzl   Awww  that s a bummer   You shoulda got David Carr of Third Day to do it   D', tokens=['http', 'twitpic', 'awww', 'that', 'bummer', 'shoulda', 'david', 'carr', 'third']),\n",
              " Row(No=0, id='1467810672', datetime='Mon Apr 06 22:19:49 PDT 2009', query='NO_QUERY', user='scotthamilton', text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", processed_text='s upset that he can t update his Facebook by texting it  and might cry as a result  School today also  Blah ', tokens=['upset', 'that', 'update', 'facebook', 'texting', 'might', 'result', 'school', 'today', 'also', 'blah']),\n",
              " Row(No=0, id='1467810917', datetime='Mon Apr 06 22:19:53 PDT 2009', query='NO_QUERY', user='mattycus', text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', processed_text=' I dived many times for the ball  Managed to save    The rest go out of bounds', tokens=['dived', 'many', 'times', 'ball', 'managed', 'save', 'rest', 'bounds']),\n",
              " Row(No=0, id='1467811184', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='ElleCTF', text='my whole body feels itchy and like its on fire ', processed_text='my whole body feels itchy and like its on fire ', tokens=['whole', 'body', 'feels', 'itchy', 'like', 'fire']),\n",
              " Row(No=0, id='1467811193', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='Karoli', text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", processed_text=' no  it s not behaving at all  i m mad  why am i here  because I can t see you all over there  ', tokens=['behaving', 'here', 'because', 'over', 'there'])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords removal\n",
        "stopWordsRemover.setInputCol(\"tokens\")\n",
        "stopWordsRemover.setOutputCol(\"final_tokens\")\n",
        "tokenized_tweets_2 = stopWordsRemover.transform(tokenized_tweets)"
      ],
      "metadata": {
        "id": "RgTs84quqPsl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweets_2.printSchema()\n",
        "tokenized_tweets_2.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqQp9MFJsAiq",
        "outputId": "df8e140c-4ee5-4abe-d28f-39fd39b6e4a7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- No: integer (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- datetime: string (nullable = true)\n",
            " |-- query: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- processed_text: string (nullable = true)\n",
            " |-- tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- final_tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(No=0, id='1467810369', datetime='Mon Apr 06 22:19:45 PDT 2009', query='NO_QUERY', user='_TheSpecialOne_', text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", processed_text=' http twitpic com yzl   Awww  that s a bummer   You shoulda got David Carr of Third Day to do it   D', tokens=['http', 'twitpic', 'awww', 'that', 'bummer', 'shoulda', 'david', 'carr', 'third'], final_tokens=['twitpic', 'awww', 'bummer', 'shoulda', 'david', 'carr', 'third']),\n",
              " Row(No=0, id='1467810672', datetime='Mon Apr 06 22:19:49 PDT 2009', query='NO_QUERY', user='scotthamilton', text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", processed_text='s upset that he can t update his Facebook by texting it  and might cry as a result  School today also  Blah ', tokens=['upset', 'that', 'update', 'facebook', 'texting', 'might', 'result', 'school', 'today', 'also', 'blah'], final_tokens=['upset', 'update', 'facebook', 'texting', 'might', 'result', 'school', 'today', 'also', 'blah']),\n",
              " Row(No=0, id='1467810917', datetime='Mon Apr 06 22:19:53 PDT 2009', query='NO_QUERY', user='mattycus', text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', processed_text=' I dived many times for the ball  Managed to save    The rest go out of bounds', tokens=['dived', 'many', 'times', 'ball', 'managed', 'save', 'rest', 'bounds'], final_tokens=['dived', 'many', 'times', 'ball', 'managed', 'save', 'rest', 'bounds']),\n",
              " Row(No=0, id='1467811184', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='ElleCTF', text='my whole body feels itchy and like its on fire ', processed_text='my whole body feels itchy and like its on fire ', tokens=['whole', 'body', 'feels', 'itchy', 'like', 'fire'], final_tokens=['whole', 'body', 'feels', 'itchy', 'like', 'fire']),\n",
              " Row(No=0, id='1467811193', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='Karoli', text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", processed_text=' no  it s not behaving at all  i m mad  why am i here  because I can t see you all over there  ', tokens=['behaving', 'here', 'because', 'over', 'there'], final_tokens=['behaving'])]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8VCpxppjR5K",
        "outputId": "1f848dc1-8050-427d-d9f6-75745f8efb72"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(row):\n",
        "    row = [lemmatizer.lemmatize(word,'v') for word in row]\n",
        "    return row\n",
        "\n",
        "lemmatization_udf = udf(lemmatize, T.ArrayType(T.StringType()))"
      ],
      "metadata": {
        "id": "_1KGG6PNjV8m"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweets=tokenized_tweets_2.withColumn('tokens_lemma', lemmatization_udf(tokenized_tweets_2['final_tokens']))"
      ],
      "metadata": {
        "id": "RKl3d4oZkFNU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweets.printSchema()\n",
        "tokenized_tweets.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJZzDe9-kUqN",
        "outputId": "5940a0a7-46aa-4927-f43a-a8a476ada5bf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- No: integer (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- datetime: string (nullable = true)\n",
            " |-- query: string (nullable = true)\n",
            " |-- user: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- processed_text: string (nullable = true)\n",
            " |-- tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- final_tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- tokens_lemma: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(No=0, id='1467810369', datetime='Mon Apr 06 22:19:45 PDT 2009', query='NO_QUERY', user='_TheSpecialOne_', text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", processed_text=' http twitpic com yzl   Awww  that s a bummer   You shoulda got David Carr of Third Day to do it   D', tokens=['http', 'twitpic', 'awww', 'that', 'bummer', 'shoulda', 'david', 'carr', 'third'], final_tokens=['twitpic', 'awww', 'bummer', 'shoulda', 'david', 'carr', 'third'], tokens_lemma=['twitpic', 'awww', 'bummer', 'shoulda', 'david', 'carr', 'third']),\n",
              " Row(No=0, id='1467810672', datetime='Mon Apr 06 22:19:49 PDT 2009', query='NO_QUERY', user='scotthamilton', text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", processed_text='s upset that he can t update his Facebook by texting it  and might cry as a result  School today also  Blah ', tokens=['upset', 'that', 'update', 'facebook', 'texting', 'might', 'result', 'school', 'today', 'also', 'blah'], final_tokens=['upset', 'update', 'facebook', 'texting', 'might', 'result', 'school', 'today', 'also', 'blah'], tokens_lemma=['upset', 'update', 'facebook', 'texting', 'might', 'result', 'school', 'today', 'also', 'blah']),\n",
              " Row(No=0, id='1467810917', datetime='Mon Apr 06 22:19:53 PDT 2009', query='NO_QUERY', user='mattycus', text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', processed_text=' I dived many times for the ball  Managed to save    The rest go out of bounds', tokens=['dived', 'many', 'times', 'ball', 'managed', 'save', 'rest', 'bounds'], final_tokens=['dived', 'many', 'times', 'ball', 'managed', 'save', 'rest', 'bounds'], tokens_lemma=['dive', 'many', 'time', 'ball', 'manage', 'save', 'rest', 'bound']),\n",
              " Row(No=0, id='1467811184', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='ElleCTF', text='my whole body feels itchy and like its on fire ', processed_text='my whole body feels itchy and like its on fire ', tokens=['whole', 'body', 'feels', 'itchy', 'like', 'fire'], final_tokens=['whole', 'body', 'feels', 'itchy', 'like', 'fire'], tokens_lemma=['whole', 'body', 'feel', 'itchy', 'like', 'fire']),\n",
              " Row(No=0, id='1467811193', datetime='Mon Apr 06 22:19:57 PDT 2009', query='NO_QUERY', user='Karoli', text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", processed_text=' no  it s not behaving at all  i m mad  why am i here  because I can t see you all over there  ', tokens=['behaving', 'here', 'because', 'over', 'there'], final_tokens=['behaving'], tokens_lemma=['behave'])]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_test_tweets = tokenized_tweets.sample(withReplacement=False, fraction=0.1)\n",
        "preprocessed_test_tweets.write.json(\"preprocessed_test_tweets\")"
      ],
      "metadata": {
        "id": "dKJZXsPtsrsL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvf preprocessed_test_tweets.tar preprocessed_test_tweets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_16G67yuC5t",
        "outputId": "c846fed8-e16a-41d7-b81c-6995b36af8fb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessed_test_tweets/\n",
            "preprocessed_test_tweets/.part-00000-6cea9b52-9afb-470f-8bd6-79ac7b024fdc-c000.json.crc\n",
            "preprocessed_test_tweets/._SUCCESS.crc\n",
            "preprocessed_test_tweets/part-00000-6cea9b52-9afb-470f-8bd6-79ac7b024fdc-c000.json\n",
            "preprocessed_test_tweets/_SUCCESS\n",
            "preprocessed_test_tweets/.part-00001-6cea9b52-9afb-470f-8bd6-79ac7b024fdc-c000.json.crc\n",
            "preprocessed_test_tweets/part-00001-6cea9b52-9afb-470f-8bd6-79ac7b024fdc-c000.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic modelling with grid search for the number of topics**"
      ],
      "metadata": {
        "id": "YBdyE0dTncP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml.feature import CountVectorizer"
      ],
      "metadata": {
        "id": "tUjA5MEGpZu3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countVectorizer = CountVectorizer()\n",
        "countVectorizer.setInputCol(\"tokens_lemma\")\n",
        "countVectorizer.setOutputCol(\"features\")\n",
        "vectorizerModel = countVectorizer.fit(tokenized_tweets)\n",
        "wordsVector = vectorizerModel.transform(tokenized_tweets)"
      ],
      "metadata": {
        "id": "82KCaU2ntCae"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordsVector.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQsM-9HIzjJz",
        "outputId": "9b05eb46-8e2f-464e-a416-ca5f4c66ce64"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| No|        id|            datetime|   query|           user|                text|      processed_text|              tokens|        final_tokens|        tokens_lemma|            features|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...| http twitpic com...|[http, twitpic, a...|[twitpic, awww, b...|[twitpic, awww, b...|(262144,[56,277,5...|\n",
            "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|s upset that he c...|[upset, that, upd...|[upset, update, f...|[upset, update, f...|(262144,[6,55,141...|\n",
            "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...| I dived many tim...|[dived, many, tim...|[dived, many, tim...|[dive, many, time...|(262144,[7,169,26...|\n",
            "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|my whole body fee...|[whole, body, fee...|[whole, body, fee...|[whole, body, fee...|(262144,[3,15,258...|\n",
            "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...| no  it s not beh...|[behaving, here, ...|          [behaving]|            [behave]|(262144,[3320],[1...|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# num_topics=range(3,8)\n",
        "# models=[]\n",
        "# log_likeli=[]\n",
        "# log_perp=[]\n",
        "# for num in num_topics:\n",
        "#   lda = LDA(k=num, maxIter=50)\n",
        "#   ldaModel = lda.fit(wordsVector)\n",
        "#   models.append(ldaModel)\n",
        "#   ll = ldaModel.logLikelihood(wordsVector)\n",
        "#   lp = ldaModel.logPerplexity(wordsVector)\n",
        "#   log_likeli.append(ll)\n",
        "#   log_perp.append(lp)"
      ],
      "metadata": {
        "id": "D_5-EPdhnbAb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# plot_data=pd.DataFrame(list(zip(num_topics,log_likeli,log_perp)),\n",
        "#             columns=['topics_num','logLikelihood','logPerplexity'])    \n",
        "\n",
        "# plot_data.plot(x='topics_num',y='logLikelihood',kind = 'line')\n",
        "# plt.show()\n",
        "\n",
        "# plot_data.plot(x='topics_num',y='logPerplexity',kind = 'line')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "JR5C9ke3Uh1n"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LDA(k=6, maxIter=100)\n",
        "lda_model = lda.fit(wordsVector)"
      ],
      "metadata": {
        "id": "witM8HYey0eE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting vocabulary from CountVectorizer\n",
        "vocabulary = vectorizerModel.vocabulary\n",
        "\n",
        "# create topics based on LDA\n",
        "lda_topics = lda_model.describeTopics()\n",
        "lda_topics.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2h6tuOnzYN7",
        "outputId": "68e68816-0ae1-4e3e-d798-9091968dc3ba"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|topic|         termIndices|         termWeights|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|[15, 0, 3, 56, 14...|[0.02218071747784...|\n",
            "|    1|[1, 0, 3, 2, 7, 8...|[0.01212319713823...|\n",
            "|    2|[12, 2, 13, 88, 2...|[0.04607247712742...|\n",
            "|    3|[4897, 6422, 751,...|[9.97018451255729...|\n",
            "|    4|[5, 228, 938, 19,...|[0.17156044268459...|\n",
            "|    5|[10, 61, 25, 570,...|[0.26999900621882...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topics_words = lda_topics.rdd\\\n",
        "       .map(lambda row: row['termIndices'])\\\n",
        "       .map(lambda idx_list: [vocabulary[idx] for idx in idx_list])\\\n",
        "       .collect()\n",
        "\n",
        "for idx, topic in enumerate(topics_words):\n",
        "    print(\"topic: {}\".format(idx))\n",
        "    print(\"*\"*25)\n",
        "    for word in topic:\n",
        "       print(word, ',', end='')\n",
        "    print('')\n",
        "    print(\"*\"*25)\n",
        "    \n",
        "transformed = lda_model.transform(wordsVector)\n",
        "transformed.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyM8RYGJ1AJj",
        "outputId": "2574346c-6f5f-483f-9ac1-36c8213d96f5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: 0\n",
            "*************************\n",
            "feel ,good ,like ,twitpic ,make ,hurt ,today ,want ,morning ,rain ,\n",
            "*************************\n",
            "topic: 1\n",
            "*************************\n",
            "go ,good ,like ,work ,time ,think ,love ,know ,really ,today ,\n",
            "*************************\n",
            "topic: 2\n",
            "*************************\n",
            "miss ,work ,back ,bore ,home ,love ,today ,come ,school ,phone ,\n",
            "*************************\n",
            "topic: 3\n",
            "*************************\n",
            "mobypicture ,yesh ,tumblr ,mein ,merge ,orkut ,dolly ,mich ,nahi ,naruto ,\n",
            "*************************\n",
            "topic: 4\n",
            "*************************\n",
            "quot ,tinyurl ,hannah ,watch ,diversity ,montana ,free ,goodmorning ,angels ,google ,\n",
            "*************************\n",
            "topic: 5\n",
            "*************************\n",
            "thank ,follow ,much ,appreciate ,shout ,blip ,mention ,friday ,follower ,retweet ,\n",
            "*************************\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| No|        id|            datetime|   query|           user|                text|      processed_text|              tokens|        final_tokens|        tokens_lemma|            features|   topicDistribution|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...| http twitpic com...|[http, twitpic, a...|[twitpic, awww, b...|[twitpic, awww, b...|(262144,[56,277,5...|[0.13566244276979...|\n",
            "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|s upset that he c...|[upset, that, upd...|[upset, update, f...|[upset, update, f...|(262144,[6,55,141...|[0.01249644918950...|\n",
            "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...| I dived many tim...|[dived, many, tim...|[dived, many, tim...|[dive, many, time...|(262144,[7,169,26...|[0.01527584294276...|\n",
            "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|my whole body fee...|[whole, body, fee...|[whole, body, fee...|[whole, body, fee...|(262144,[3,15,258...|[0.82607747996160...|\n",
            "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...| no  it s not beh...|[behaving, here, ...|          [behaving]|            [behave]|(262144,[3320],[1...|[0.06478328557609...|\n",
            "|  0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...| not the whole crew |       [whole, crew]|       [whole, crew]|       [whole, crew]|(262144,[258,1514...|[0.04427184285166...|\n",
            "|  0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |         Need a hug |              [need]|              [need]|              [need]| (262144,[22],[1.0])|[0.06498700592664...|\n",
            "|  0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...| hey  long time n...|[long, time, rain...|[long, time, rain...|[long, time, rain...|(262144,[7,10,72,...|[0.02288328862838...|\n",
            "|  0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...| nope they didn t...|[nope, they, didn...|              [nope]|              [nope]|(262144,[523],[1.0])|[0.06478112403542...|\n",
            "|  0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|     que me muera   |             [muera]|             [muera]|             [muera]|(262144,[135301],...|[0.06477871049917...|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed = lda_model.transform(wordsVector)\n",
        "transformed.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUzNm6vvd2gI",
        "outputId": "6679f19d-44b7-40e1-b90a-8f1cf96d69fa"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| No|        id|            datetime|   query|           user|                text|      processed_text|              tokens|        final_tokens|        tokens_lemma|            features|   topicDistribution|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...| http twitpic com...|[http, twitpic, a...|[twitpic, awww, b...|[twitpic, awww, b...|(262144,[56,277,5...|[0.13566244276979...|\n",
            "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|s upset that he c...|[upset, that, upd...|[upset, update, f...|[upset, update, f...|(262144,[6,55,141...|[0.01249644918950...|\n",
            "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...| I dived many tim...|[dived, many, tim...|[dived, many, tim...|[dive, many, time...|(262144,[7,169,26...|[0.01527584294276...|\n",
            "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|my whole body fee...|[whole, body, fee...|[whole, body, fee...|[whole, body, fee...|(262144,[3,15,258...|[0.82607747996160...|\n",
            "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...| no  it s not beh...|[behaving, here, ...|          [behaving]|            [behave]|(262144,[3320],[1...|[0.06478328557609...|\n",
            "|  0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...| not the whole crew |       [whole, crew]|       [whole, crew]|       [whole, crew]|(262144,[258,1514...|[0.04427184285166...|\n",
            "|  0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |         Need a hug |              [need]|              [need]|              [need]| (262144,[22],[1.0])|[0.06498700592664...|\n",
            "|  0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...| hey  long time n...|[long, time, rain...|[long, time, rain...|[long, time, rain...|(262144,[7,10,72,...|[0.02288328862838...|\n",
            "|  0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...| nope they didn t...|[nope, they, didn...|              [nope]|              [nope]|(262144,[523],[1.0])|[0.06478112403542...|\n",
            "|  0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|     que me muera   |             [muera]|             [muera]|             [muera]|(262144,[135301],...|[0.06477871049917...|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.save(\"lda_model2\")"
      ],
      "metadata": {
        "id": "mpGhlqkrfn0T"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvf lda_model2.tar lda_model2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkMl_TDFgaTI",
        "outputId": "5309f65b-3423-41ea-a5ac-165fb386b8cd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lda_model2/\n",
            "lda_model2/data/\n",
            "lda_model2/data/._SUCCESS.crc\n",
            "lda_model2/data/part-00000-79228ce9-77b6-49a0-ac51-3488e36c120a-c000.snappy.parquet\n",
            "lda_model2/data/_SUCCESS\n",
            "lda_model2/data/.part-00000-79228ce9-77b6-49a0-ac51-3488e36c120a-c000.snappy.parquet.crc\n",
            "lda_model2/metadata/\n",
            "lda_model2/metadata/._SUCCESS.crc\n",
            "lda_model2/metadata/part-00000\n",
            "lda_model2/metadata/_SUCCESS\n",
            "lda_model2/metadata/.part-00000.crc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordsVector.write.json(\"processed_data\")"
      ],
      "metadata": {
        "id": "n1HRwHU_hYWr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvf processed_data.tar processed_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnxtsu3AjDAj",
        "outputId": "089a6372-3b3a-4de3-8291-d2a9f665a0af"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed_data/\n",
            "processed_data/part-00000-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json\n",
            "processed_data/part-00001-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json\n",
            "processed_data/.part-00000-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json.crc\n",
            "processed_data/.part-00001-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json.crc\n",
            "processed_data/._SUCCESS.crc\n",
            "processed_data/_SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWhj9nPyqRMe",
        "outputId": "4f90e6db-3b23-4d73-a554-901e58ef0ec9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t\tpreprocessed_test_tweets      spark-3.2.1-bin-hadoop2.7\n",
            "lda_model2\tpreprocessed_test_tweets.tar  spark-3.2.1-bin-hadoop2.7.tgz\n",
            "lda_model2.tar\tsample_data\t\t      training.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_eUBExB1BreN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LocalLDAModel, LDAModel, LDA"
      ],
      "metadata": {
        "id": "BJk71LVyC34Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvf processed_data.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwOVGNdiDZKF",
        "outputId": "1dc79faa-f94f-43dc-90a7-8066d92c47f9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed_data/\n",
            "processed_data/part-00000-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json\n",
            "processed_data/part-00001-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json\n",
            "processed_data/.part-00000-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json.crc\n",
            "processed_data/.part-00001-b0ac3ea1-1b94-42e4-a463-cd009ba86bdc-c000.json.crc\n",
            "processed_data/._SUCCESS.crc\n",
            "processed_data/_SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = LocalLDAModel.load(\"lda_model\")"
      ],
      "metadata": {
        "id": "wHQvhNpODIkb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_df = spark.read.json('processed_data')"
      ],
      "metadata": {
        "id": "pEVNgjGkEB-g"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RmoRO53U02V",
        "outputId": "1466b24a-8780-41f1-9fb2-c18d5d1db411"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+----------+--------------------+--------+--------------------+--------------------+--------------------+---------------+\n",
            "| No|            datetime|            features|        id|      processed_text|   query|                text|              tokens|        tokens_lemma|           user|\n",
            "+---+--------------------+--------------------+----------+--------------------+--------+--------------------+--------------------+--------------------+---------------+\n",
            "|  0|Mon Apr 06 22:19:...|{[2, 6, 7, 17, 24...|1467810369| http twitpic com...|NO_QUERY|@switchfoot http:...|[http, twitpic, c...|[http, twitpic, c...|_TheSpecialOne_|\n",
            "|  0|Mon Apr 06 22:19:...|{[1, 7, 13, 25, 1...|1467810672|s upset that he c...|NO_QUERY|is upset that he ...|[upset, that, can...|[upset, that, can...|  scotthamilton|\n",
            "|  0|Mon Apr 06 22:19:...|{[0, 5, 21, 27, 2...|1467810917| I dived many tim...|NO_QUERY|@Kenichan I dived...|[dived, many, tim...|[dive, many, time...|       mattycus|\n",
            "|  0|Mon Apr 06 22:19:...|{[1, 20, 43, 57, ...|1467811184|my whole body fee...|NO_QUERY|my whole body fee...|[whole, body, fee...|[whole, body, fee...|        ElleCTF|\n",
            "|  0|Mon Apr 06 22:19:...|{[2, 11, 13, 19, ...|1467811193| no  it s not beh...|NO_QUERY|@nationwideclass ...|[not, behaving, a...|[not, behave, all...|         Karoli|\n",
            "+---+--------------------+--------------------+----------+--------------------+--------+--------------------+--------------------+--------------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = loaded_df.sample(withReplacement=False, fraction=0.1)"
      ],
      "metadata": {
        "id": "BfJFar0bU9MF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.write.json('test_df')"
      ],
      "metadata": {
        "id": "n0eLwNQkVvs-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar cvf test_df.tar test_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTMMj-K8WTfe",
        "outputId": "c448ae8a-6a43-499c-933f-138e78c580a9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_df/\n",
            "test_df/.part-00003-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json.crc\n",
            "test_df/.part-00005-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json.crc\n",
            "test_df/part-00005-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json\n",
            "test_df/.part-00006-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json.crc\n",
            "test_df/part-00001-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json\n",
            "test_df/._SUCCESS.crc\n",
            "test_df/part-00000-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json\n",
            "test_df/part-00004-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json\n",
            "test_df/.part-00000-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json.crc\n",
            "test_df/part-00003-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json\n",
            "test_df/.part-00001-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json.crc\n",
            "test_df/part-00006-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json\n",
            "test_df/.part-00002-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json.crc\n",
            "test_df/part-00002-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json\n",
            "test_df/.part-00004-b23be4ea-b61d-4f52-b20e-cf5aa86ed581-c000.json.crc\n",
            "test_df/_SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.select(['features']).head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWuxE_kkgbNQ",
        "outputId": "202cebeb-af89-41a0-ec1a-816fccf7a8b8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(features=Row(indices=[2061, 205972], size=262144, type=0, values=[1.0, 1.0])),\n",
              " Row(features=Row(indices=[1, 9, 16, 54, 71, 128, 271, 281, 2779], size=262144, type=0, values=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])),\n",
              " Row(features=Row(indices=[0, 1, 5, 11, 15, 17, 20, 64, 147, 191, 243, 360, 593, 616, 1445], size=262144, type=0, values=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0])),\n",
              " Row(features=Row(indices=[7, 17, 18, 36, 73, 102, 110, 173, 314, 546, 578, 1446], size=262144, type=0, values=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0])),\n",
              " Row(features=Row(indices=[0, 4, 5, 40, 88, 132, 185, 351, 440, 506, 871, 13259], size=262144, type=0, values=[3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]))]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction = loaded_model.transform(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "dD38UM2u9trQ",
        "outputId": "2e967d53-e200-4273-bcad-d97bccff08d4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b4019f55248e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column features must be of type equal to one of the following types: [struct<type:tinyint,size:int,indices:array<int>,values:array<double>>, array<double>, array<float>] but was actually of type struct<indices:array<bigint>,size:bigint,type:bigint,values:array<double>>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction.select(['text', 'features', 'topicDistribution']).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "GNGWMYhIXDNr",
        "outputId": "427ee9b7-0fc8-461c-ab03-b412e6042f03"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-27681dbd628b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'topicDistribution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_prediction' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "twitter_topics_modelling.ipynb",
      "provenance": [],
      "mount_file_id": "1LrHvLtuROSvUjeW9qEFgUx4v4yGWYPVk",
      "authorship_tag": "ABX9TyP4mH6Ib/hBACqqbegVviqj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}